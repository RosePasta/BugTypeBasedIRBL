<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0"  xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="scripting">

    <title>Working with the Hadoop File System</title>
    
    <para>A common task in Hadoop is interacting with its file system, whether for provisioning, adding new files to be processed, parsing results, or performing cleanup. Hadoop offers several ways to achieve that:
    one can use its Java API (namely <ulink url="http://hadoop.apache.org/common/docs/stable/api/index.html?org/apache/hadoop/fs/FileSystem.html"><literal>FileSystem</literal></ulink>) or 
    use the <literal>hadoop</literal> command line, in particular the file system <ulink url="http://hadoop.apache.org/common/docs/stable/file_system_shell.html">shell</ulink>. However there is no middle ground,
    one either has to use the (somewhat verbose, full of checked exceptions) API or fall back to the command line, outside the application. SHDP addresses this issue by bridging the two worlds, exposing both the 
    <literal>FileSystem</literal> and the fs shell through an intuitive, easy-to-use Java API. Add your favorite <ulink url="http://en.wikipedia.org/wiki/List_of_JVM_languages">JVM scripting</ulink> language right 
    inside your Spring Hadoop application and you have a powerful combination.</para>
    
    <section id="scripting:api">
    	<title>Scripting the Hadoop API</title>
    	
    	<sidebar>
    	  <title>Supported scripting languages</title>
    	  
    	  <para>SHDP scripting supports any <ulink url="http://www.jcp.org/en/jsr/detail?id=223">JSR-223</ulink> (also known as <literal>javax.scripting</literal>) compliant scripting engine. Simply add the 
    	  engine jar to the classpath and the application should be able to find it. Most languages (such as Groovy or JRuby) provide JSR-233 support out of the box; for those that do not see the 
    	  <ulink url="http://java.net/projects/scripting">scripting</ulink> project that provides various adapters.</para>
    	</sidebar>
    
    	<para>Since Hadoop is written in Java, accessing its APIs in a <emphasis>native</emphasis> way provides maximum control and flexibility over the interaction with Hadoop. This holds true for working with
    	its files system; in fact all the other tools that one might use are built upon these. The main entry point is the <classname>org.apache.hadoop.fs.FileSystem</classname> abstract class which provides the
    	foundation of most (if not all) of the actual file system implementations out there. Whether one is using a local, remote or distributed store through the <classname>FileSystem</classname> API she
    	can query and manipulate the available resources or create new ones. To do so however, one needs to write Java code, compile the classes and configure them which is somewhat cumbersome especially when
    	performing simple, straight-forward operations (like copy a file or delete a directory).</para>
    	
    	<para>JVM scripting languages (such as <ulink url="http://groovy.codehaus.org/">Groovy</ulink>, <ulink url="http://jruby.org/">JRuby</ulink>, <ulink url="http://www.jython.org/">Jython</ulink> or 
    	<ulink url="http://www.mozilla.org/rhino/">Rhino</ulink> to name just a few) provide a nice solution to the Java language; they run on the JVM, can interact with the Java code with no or few
    	changes or restrictions and have a nicer, simpler, less <emphasis>ceremonial</emphasis> syntax; that is, there is no need to define a class or a method - simply write the code that you want to execute and you are done.
    	SHDP combines the two, taking care of the configuration and the infrastructure so one can interact with the Hadoop environment from her language of choice</para>
    	
    	<para>Let us take a look of a JavaScript example using Rhino (which is part of JDK 6 or higher, meaning one does not need any extra libraries):</para>

		<programlisting language="xml"><![CDATA[<beans xmlns="http://www.springframework.org/schema/beans" ...>		
  <hdp:configuration .../>
		
  <hdp:script id="inlined-js" language="javascript">
    importPackage(java.util);

    name = UUID.randomUUID().toString()
    scriptName = "src/test/resources/test.properties"
    ]]>// <emphasis role="strong">fs</emphasis> - FileSystem instance based on 'hadoop-configuration' bean
    // call FileSystem#copyFromLocal(Path, Path)  
    <emphasis role="strong">fs</emphasis>.copyFromLocalFile(scriptName, name)
    // return the file length 
    <emphasis role="strong">fs</emphasis>.getLength(name)<![CDATA[
  </hdp:script>
	 
</beans>]]></programlisting>

		<para>The <literal>script</literal> element, part of the SHDP namespace, builds on top of the scripting support in Spring permitting script declarations to be evaluated and declared as normal bean definitions. Further
		more it automatically exposes Hadoop-specific objects, based on the existing configuration, to the script such as the <literal>FileSystem</literal> (more on that in the next section). As one can see, the script
		is fairly obvious: it generates a random name (using the <classname>UUID</classname> class from <literal>java.util</literal> package) and the copies a local file into HDFS under the random name. The last line returns
		the length of the copied file which becomes the value of the declaring bean (in this case <literal>inlined-js</literal>) - note that this might vary based on the scripting engine used.</para>
		
		<note>The attentive reader might have noticed that the arguments passed to the <literal>FileSystem</literal> object are not of type <literal>Path</literal> but rather <literal>String</literal>. To avoid
		the creation of <literal>Path</literal> object, SHDP uses a wrapper class (<literal>SimplerFileSystem</literal>) which automatically does the conversion so you don't have to. For more information see the 
		<link linkend="scripting:vars">implicit variables</link> section.</note>
		<para>Note that for inlined scripts, one can use Spring's property placeholder configurer to automatically expand variables at runtime. Using one of the examples before:</para>
		
		<programlisting language="xml"><![CDATA[<beans ...>
  <context:property-placeholder location="classpath:hadoop.properties" />
   
  <hdp:script language="javascript">
    ...
    tracker=]]><emphasis role="strong">${hd.fs}</emphasis><![CDATA[
    ...
  </hdp:script>
</beans>]]></programlisting>

		<para>Notice how the script above relies on the property placeholder to expand <literal>${hd.fs}</literal> with the values from <literal>hadoop.properties</literal> file available in the classpath.</para>
		
		<section id="scripting:api:scripts">
			<title>Using scripts</title>
			
			<para>Inlined scripting is quite handy for doing simple operations and couple with the property expansion is quite a powerful tool that can handle a variety of use cases. However when more logic is required
or the script is affected by XML formatting, encoding or syntax restrictions (such as Jython/Python for which white-spaces are important) one should consider externalization. That is rather then declaring the script
directly inside the XML, one can declare it in its own file. And speaking of Python, consider the variation of the previous example:</para>

			<programlisting language="xml"><![CDATA[<hdp:script location="org/company/basic-script.py"/>]]></programlisting>
			
			<para>The definition does not bring any surprises but do notice there is no need to specify the language (as in the case of a inlined declaration) 
			since script extension (<literal>py</literal>) already provides that information. Just for completeness, the <literal>basic-script.py</literal> looks as follows:</para>
			
			<programlisting language="python"><![CDATA[from java.util import UUID
from org.apache.hadoop.fs import Path

print "Home dir is " + str(fs.homeDirectory)
print "Work dir is " + str(fs.workingDirectory)
print "/user exists " + str(fs.exists("/user"))

name = UUID.randomUUID().toString()
scriptName = "src/test/resources/test.properties"
fs.copyFromLocalFile(scriptName, name)
print Path(name).makeQualified(fs)]]></programlisting>

		</section>
	</section>
	
	<section id="scripting:vars">
		<title>Scripting implicit variables</title>
		
		<para>To ease the interaction of the script with its enclosing context, SHDP binds by default the so-called <emphasis>implicit</emphasis> variables. These are:</para>
		<table id="scripting:vars:tbl" pgwide="1" align="center">
			<title>Implicit variables</title>
		
			<tgroup cols="3">
    		 <colspec colname="c1" colwidth="1*"/>
    		 <colspec colname="c2" colwidth="1*"/>
    		 <colspec colname="c3" colwidth="4*"/>
          	 <spanspec spanname="description" namest="c2" nameend="c3" align="center"/>
          		
          	  <thead>
          	   <row>
          	     <entry>Name</entry>
          	     <entry>Type</entry>
          	     <entry align="center">Description</entry>
          	   </row>
          	  </thead>
          	  
          	  <tbody>
          	    <row>
          	      <entry>cfg</entry>
          	      <entry><literal>org.apache.hadoop.conf.Configuration</literal></entry>
          	      <entry>Hadoop Configuration (relies on <emphasis>hadoop-configuration</emphasis> bean or singleton type match)</entry>
          	    </row>
          	    <row>
          	      <entry>cl</entry>
          	      <entry><literal>java.lang.ClassLoader</literal></entry>
          	      <entry>ClassLoader used for executing the script</entry>
          	    </row>
          	    <row>
          	      <entry>ctx</entry>
          	      <entry><literal>org.springframework.context.ApplicationContext</literal></entry>
          	      <entry>Enclosing application context</entry>
          	    </row>
          	    <row>
          	      <entry>ctxRL</entry>
          	      <entry><literal>org.springframework.io.support.ResourcePatternResolver</literal></entry>
          	      <entry>Enclosing application context ResourceLoader</entry>
          	    </row>
          	    <row>
          	      <entry>distcp</entry>
          	      <entry><literal>org.springframework.data.hadoop.fs.DistributedCopyUtil</literal></entry>
          	      <entry>Programmatic access to DistCp</entry>
          	    </row>
          	    <row>
          	      <entry>fs</entry>
          	      <entry><literal>org.apache.hadoop.fs.FileSystem</literal></entry>
          	      <entry>Hadoop File System (relies on 'hadoop-fs' bean or singleton type match, falls back to creating one based on 'cfg')</entry>
          	    </row>
          	    <row>
          	      <entry>fsh</entry>
          	      <entry><literal>org.springframework.data.hadoop.fs.FsShell</literal></entry>
          	      <entry>File System shell, exposing hadoop 'fs' commands as an API</entry>
          	    </row>
          	    <row>
          	      <entry>hdfsRL</entry>
          	      <entry><literal>org.springframework.data.hadoop.io.HdfsResourceLoader</literal></entry>
          	      <entry>Hdfs resource loader (relies on 'hadoop-resource-loader' or singleton type match, falls back to creating one automatically based on 'cfg')</entry>
          	    </row>
          	  </tbody>
          	</tgroup>  
		</table>
		
		<para>As mentioned in the <emphasis>Description</emphasis> column, the variables are first looked (either by name or by type) in the application context and, in case they are missing, created on the spot based on
		the existing configuration. Note that it is possible to override or add new variables to the scripts through the <literal>property</literal> sub-element that can set values or references to other beans:</para>
		
		<programlisting language="xml"><![CDATA[<hdp:script location="org/company/basic-script.js">
   <hdp:property name="foo" value="bar"/>
   <hdp:property name="ref" ref="some-bean"/>
</hdp:script>]]></programlisting>
	
    </section>
    
    <section id="scripting-fssh">
    	<title>File System Shell (FsShell)</title>
    	
    	<para>A handy utility provided by the Hadoop distribution is the file system <ulink url="http://hadoop.apache.org/common/docs/stable/file_system_shell.html">shell</ulink> which allows UNIX-like commands to be
    	executed against HDFS. One can check for the existance of files, delete, move, copy directories or files or setting up permissions. However the utility is only available from the command-line which makes it hard
    	to use it from/inside a Java application. To address this problem, SHDP provides a lightweight, fully embeddable shell, called <literal>FsShell</literal> which mimics most of the commands available from the command line:
    	rather then dealing with the <literal>System.in</literal> or <literal>System.out</literal>, one deals with objects.</para>
    	
    	<para>Let us take a look of using <literal>FsShell</literal> by building on the previous scripting examples:</para>
    	
    	<programlisting language="xml"><![CDATA[<hdp:script location="org/company/basic-script.groovy"/>]]></programlisting>
    	
		<programlisting language="java" linenumbering="numbered"><![CDATA[name = UUID.randomUUID().toString()
scriptName = "src/test/resources/test.properties"
fs.copyFromLocalFile(scriptName, name)

// use the shell (made available under variable ]]><emphasis role="strong">fsh</emphasis> 
dir = "script-dir"
if (!fsh.test(dir)) {
   fsh.mkdir(dir); fsh.cp(name, dir); fsh.chmodr(700, dir)
   println "File content is " + fsh.cat(dir + name).toString()
}
println fsh.ls(dir).toString()
fsh.rmr(dir)
</programlisting>

		<para>As mentioned in the previous section, a <literal>FsShell</literal> instance is automatically created and for configured for scripts, under the name <emphasis>fsh</emphasis>. 
		Notice how the entire block relies on the usual	commands: <literal>test</literal>, <literal>mkdir</literal>, <literal>cp</literal> and so on. 
		Their semantics are exactly the same as in the command-line version however one has access to a native Java API that returns
		actual objects (rather then <literal>String</literal>s) making it easy to use them programmatically whether in Java or another language. Further more, the class offers enhanced methods (such as <literal>chmodr</literal>
		which stands for <emphasis>recursive</emphasis> <literal>chmod</literal>) and multiple overloaded methods taking advantage of <ulink url="http://docs.oracle.com/javase/1.5.0/docs/guide/language/varargs.html">varargs</ulink> 
		so that multiple parameters can be specified. Consult the <ulink url="http://static.springsource.org/spring-hadoop/docs/current/api/index.html?org/springframework/data/hadoop/fs/FsShell.html">API</ulink> for more information.
		</para>
		
		<para>To be as close as possible to the command-line shell, <literal>FsShell</literal> mimics even the messages being displayed. Take a look at line 9 which prints the result of <literal>fsh.cat()</literal>. The
		method returns a <literal>Collection</literal> of Hadoop <literal>Path</literal> objects (which one can use programatically). However when invoking <literal>toString</literal> on the collection, the same printout as from
		the command-line shell is being displayed:</para>
		
		<programlisting><![CDATA[File content is ]]><emphasis>some text</emphasis></programlisting>
		
		<para>The same goes for the rest of the methods, such as <literal>ls</literal>. The same script in JRuby would look something like this:</para>

		<programlisting language="ruby" linenumbering="numbered"><![CDATA[require 'java'
name = java.util.UUID.randomUUID().to_s
scriptName = "src/test/resources/test.properties"
$fs.copyFromLocalFile(scriptName, name)

# use the shell
dir = "script-dir/"
...
print $fsh.ls(dir).to_s]]></programlisting>

		<para>which prints out something like this:</para>
		
		<programlisting><![CDATA[drwx------   - user     supergroup          0 2012-01-26 14:08 /user/user/script-dir
-rw-r--r--   3 user     supergroup        344 2012-01-26 14:08 /user/user/script-dir/520cf2f6-a0b6-427e-a232-2d5426c2bc4e]]></programlisting>

		<para>As you can see, not only you can reuse the existing tools and commands with Hadoop inside SHDP, but you can also code against them in various scripting languages. And as you might have noticed, there is no
		special configuration required - this is automatically inferred from the enclosing application context.</para>
		
		<note>The careful reader might have noticed that besides the syntax, there are some minor differences in how the various langauges interact with the java objects. For example the automatic <literal>toString</literal>
		call called in Java for doing automatic <literal>String</literal> conversion is not necessarily supported (hence the <literal>to_s</literal> in Ruby or <literal>str</literal> in Python). This is to be expected
		as each language has its own semantics - for the most part these are easy to pick up but do pay attention to details.</note>


		<section id="scripting:fssh:distcp">
			<title>DistCp API</title>
			
			<para>Similar to the <literal>FsShell</literal>, SHDP provides a lightweight, fully embeddable <ulink url="http://hadoop.apache.org/common/docs/stable/distcp.html"><literal>DistCp</literal></ulink> version 
			that builds on top of the <literal>distcp</literal> from the Hadoop distro.	The semantics are configuration options are the same however, one can use it from within an Java application without having to use 
			the command-line. See the <ulink url="http://static.springsource.org/spring-hadoop/docs/current/api/index.html?org/springframework/data/hadoop/fs/DistCp.html">API</ulink> for more information:</para>
			
			<programlisting language="xml"><![CDATA[<hdp:script language="groovy">distcp.copy("${distcp.src}", "${distcp.dst}")</hdp:script>]]></programlisting>
			
			<para>The bean above triggers a distributed copy relying again on Spring's property placeholder variable expansion for its source and destination.</para>
		</section>
    </section>

	<section id="scripting:lifecycle">
		<title>Scripting Lifecycle</title>
		
		<para>The <literal>script</literal> namespace provides various options to adjust its behaviour depending on the script content. By default the script is executed in a lazy manner - that is when the declaring
		bean is being referred/used by another entity. One however can change that so that the script gets evaluated at startup through the <literal>run-at-startup</literal> flag (which is by default <literal>false</literal>).
		Similarily, by default the script gets evaluated every single time the bean is being invoked - that is the script is actually ran every time one refers to it. However for scripts that are expensive and return the same
		value every time one has various <emphasis>caching</emphasis> options, so the evaluation occurs only when needed through the <literal>evaluate</literal> attribute:</para>
		
		<table id="scripting:lifecycle:flags" pgwide="1" align="center">
			<title><literal>script</literal> attributes</title>
		
			<tgroup cols="3">
    		 <colspec colname="c1" colwidth="1*"/>
    		 <colspec colname="c2" colwidth="1*"/>
    		 <colspec colname="c3" colwidth="4*"/>
          	 <spanspec spanname="description" namest="c2" nameend="c3" align="center"/>
          		
          	  <thead>
          	   <row>
          	     <entry>Name</entry>
          	     <entry>Values</entry>
          	     <entry align="center">Description</entry>
          	   </row>
          	  </thead>
          	  
          	  <tbody>
          	    <row>
          	      <entry><literal>run-at-startup</literal></entry>
          	      <entry><literal>false</literal>(default), <literal>true</literal></entry>
          	      <entry>Wether the script is executed at startup or on demand (lazy)</entry>
          	    </row>
          	    <row>
          	      <entry><literal>evaluate</literal></entry>
          	      <entry><literal>ALWAYS</literal>(default), <literal>IF_MODIFIED</literal>, <literal>ONCE</literal></entry>
          	      <entry>Wether to actually evaluate the script when invoked or used a previous value. <literal>ALWAYS</literal> means evaluate every time, <literal>IF_MODIFIED</literal> evaluate if the backing
          	      resource (such as a file) has been modified in the meantime and <literal>ONCE</literal> only one.</entry>
          	    </row>
          	  </tbody>
          	</tgroup>  
		
		</table>
	</section>  

	<section id="scripting-tasklet">
		<title>Using the Scripting tasklet</title>

		<para>For Spring Batch environments, SHDP provides a dedicated tasklet to execute scripts.</para>
		
		<programlisting language="xml"><![CDATA[<script-tasklet id="script-tasklet">
  <script language="groovy">
    inputPath = "/user/gutenberg/input/word/"
    outputPath = "/user/gutenberg/output/word/"
    if (fsh.test(inputPath)) {
      fsh.rmr(inputPath)
    }
    if (fsh.test(outputPath)) {
      fsh.rmr(outputPath)
    }
    inputFile = "src/main/resources/data/nietzsche-chapter-1.txt"
    fsh.put(inputFile, inputPath)
  </script>
</script-tasklet>]]></programlisting>

		<para>The tasklet above embedds the script as a nested element.  You can also declare a reference to another script definition, using the script-ref attribute which allows you to externalize the scripting code to an external resource.</para>	
		<programlisting language="xml"><![CDATA[<script-tasklet id="script-tasklet" script-ref="clean-up"/>
<hdp:script id="clean-up" location="org/company/myapp/clean-up-wordcount.groovy"/>]]></programlisting>
	</section>   
</chapter>