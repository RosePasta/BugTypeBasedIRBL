buildscript {
    repositories {
        maven { url 'http://repo.spring.io/plugins-release' }
    }
    dependencies {
        classpath 'org.springframework.build.gradle:docbook-reference-plugin:0.2.4'
        classpath 'org.springframework.build.gradle:propdeps-plugin:0.0.3'
    }
}

description = 'Spring for Apache Hadoop'
group = 'org.springframework.data'

repositories {
    mavenCentral()
    maven { url "http://repo.spring.io/libs-snapshot" }
}

apply plugin: "java"
apply plugin: 'eclipse'
apply plugin: 'idea'
apply from: "$rootDir/maven.gradle"
apply plugin: 'docbook-reference'
apply plugin: 'propdeps'
apply plugin: 'propdeps-idea'
apply plugin: 'propdeps-eclipse'

// 
//  Select the Hadoop distribution used for building the binaries
// 
def List hadoop = []

def hadoopDefault = "hadoop12"
def hadoopDistro = project.hasProperty("distro") ? project.getProperty("distro") : hadoopDefault
def hadoopVersion = hd12Version

// make it possible to use Pig jars compiled for Hadoop 2.0
def pigQualifier = ''

// handle older Hive version
def hiveGroup = "org.apache.hive"

// default is Hadoop 1.2.x
switch (hadoopDistro) {

  // Cloudera CDH3
  case "cdh3":
    hadoopVersion = cdh3Version
    println "Using Cloudera CDH3 [$hadoopVersion]"
    
    hbaseVersion = cdh3HbaseVersion
    // Hive in CDH3 is too old and does not allow Hive Server to be compiled
    // Note that the POMs and repo are incomplete (hive-builtin is missing)
    // hiveVersion = cdh3HiveVersion
    // hiveGroup = "org.apache.hadoop.hive"
    pigVersion = cdh3PigVersion

    dependencies {
        optional("org.apache.hadoop:hadoop-streaming:$hadoopVersion") 
        optional("org.apache.hadoop:hadoop-tools:$hadoopVersion")
        testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
    }
    
  break;    

  // Cloudera CDH4
  case "cdh4":
    hadoopVersion = cdh4MR1Version
    println "Using Cloudera CDH4 [$hadoopVersion]"

    hbaseVersion = cdh4HbaseVersion
    hiveVersion = cdh4HiveVersion
    pigVersion = cdh4PigVersion
    thriftVersion = cdh4ThriftVersion

    dependencies {
        optional("org.apache.hadoop:hadoop-streaming:$cdh4MR1Version") 
        optional("org.apache.hadoop:hadoop-tools:$cdh4MR1Version")
        optional("org.apache.hadoop:hadoop-common:$cdh4Version")
        optional("org.apache.hadoop:hadoop-hdfs:$cdh4Version")
        testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
    }

  break;    

  // Pivotal HD 1.0
  case "phd1":
    hadoopVersion = phd1Version
    println "Using Pivotal HD 1.0 - [$hadoopVersion]"

    hbaseVersion = phd1HbaseVersion
    hiveVersion = phd1HiveVersion
    pigVersion = phd1PigVersion
    thriftVersion = phd1ThriftVersion

    hadoop = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
              "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
              "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
              "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]

    dependencies {
        compile "org.apache.hive:hive-common:$hiveVersion"
        compile "org.apache.hive:hive-metastore:$hiveVersion"
        compile "org.apache.hive:hive-exec:$hiveVersion"
        testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
        testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
    }

  break;

  // Hortonworks Data Platform 1.3
  case "hdp13":
    hadoopVersion = hdp13Version
    
    println "Using Hortonworks Data Platform 1.3 [$hadoopVersion]"
    
    hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"]

    hbaseVersion = hdp13HbaseVersion
    hiveVersion = hdp13HiveVersion
    pigVersion = hdp13PigVersion
    thriftVersion = hdp13ThriftVersion

    dependencies {
        testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
        testRuntime "dk.brics.automaton:automaton:1.11-8"
    }

  break;

  // Hadoop 2.2
  case "hadoop22":
    hadoopVersion = hd22Version
    println "Using Apache Hadoop 2.2 - [$hadoopVersion]"

    hadoop = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
              "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
              "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
              "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]

    hbaseVersion = hd22HbaseVersion
    hiveVersion = hd22HiveVersion
    pigVersion = hd22PigVersion
    pigQualifier = ':h2'
    thriftVersion = hd22ThriftVersion

    dependencies {
        testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
        testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
        testRuntime "dk.brics.automaton:automaton:1.11-8"
        testRuntime "$hiveGroup:hive-builtins:$hiveVersion" 
    }

  break;

  // Hadoop 1.1.x
  case "hadoop11":
    hadoopVersion = hd11Version
    
    println "Using Apache Hadoop 1.1.x [$hadoopVersion]"
    
    hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"]

    hbaseVersion = hd11HbaseVersion
    hiveVersion = hd11HiveVersion
    pigVersion = hd11PigVersion

    dependencies {
        testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
        testRuntime "$hiveGroup:hive-builtins:$hiveVersion" 
    }


  break;

  // Hadoop 1.0.x
  case "hadoop10":
    hadoopVersion = hd10Version

    println "Using Apache Hadoop 1.0.x [$hadoopVersion]"

    hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"]

    hbaseVersion = hd10HbaseVersion
    hiveVersion = hd10HiveVersion
    pigVersion = hd10PigVersion

    dependencies {
        testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
        testRuntime "$hiveGroup:hive-builtins:$hiveVersion" 
    }

  break;

  default:
    if (!project.hasProperty("distro")) {
        println "Using default distro: Apache Hadoop [$hadoopVersion]"
    } else {
        if (hadoopDistro == hadoopDefault) {
            println "Using Apache Hadoop 1.2.x [$hadoopVersion]"
        } else {
            println "Failing build: $hadoopDistro is not a supported distro"
            println "Supported distros: hadoop10, hadoop11, hadoop12[*], hadoop22, hdp13, chd3, chd4 and phd1"
            println "* default"
            throw new InvalidUserDataException("$hadoopDistro is not a supported distro")
        }
    }
    hadoopVersion = hd12Version
    
    hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"]

    hbaseVersion = hd12HbaseVersion
    hiveVersion = hd12HiveVersion
    pigVersion = hd12PigVersion
    thriftVersion = hd12ThriftVersion

    dependencies {
        testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
        testRuntime "$hiveGroup:hive-builtins:$hiveVersion" 
    }

}
// Common dependencies
dependencies {
    compile hadoop

    // Logging - using commons-logging from spring-core
    testRuntime("log4j:log4j:$log4jVersion")

    // Spring Framework
    // context-support -> spring-aop/beans/core -> commons-logging
    compile "org.springframework:spring-context-support:$springVersion"
    // used for DAO exceptions by Pig/HBase/Hive packages
    optional("org.springframework:spring-tx:$springVersion")
    // used by Hive package
    optional("org.springframework:spring-jdbc:$springVersion")
    // tasklet integration
    optional("org.springframework.batch:spring-batch-core:$springBatchVersion")
    // Cascading local Taps
    optional("org.springframework.integration:spring-integration-core:$springIntVersion")

    // cascading
    optional("cascading:cascading-hadoop:$cascadingVersion") { dep ->
        exclude module: "hadoop-core"
    }
    
    // Missing dependency in Hadoop 1.0.3
    testRuntime "commons-io:commons-io:$commonsioVersion"
    testRuntime "org.codehaus.jackson:jackson-mapper-asl:$jacksonVersion"
    testRuntime "cglib:cglib:$cglibVersion"

    // Hive
    optional("$hiveGroup:hive-service:$hiveVersion")
    
    // Pig
    optional("org.apache.pig:pig:$pigVersion$pigQualifier")
    
    // HBase
    optional("org.apache.hbase:hbase:$hbaseVersion") { dep ->
        exclude module: "thrift"
    }

    // Testing
 
    // needed by JDBC test
    testRuntime "$hiveGroup:hive-jdbc:$hiveVersion"
    
    // needed by the Hive Server tests
    testRuntime "org.apache.thrift:libthrift:$thriftVersion"
    testRuntime "org.apache.thrift:libfb303:$thriftVersion"

    testCompile "junit:junit:$junitVersion"
    testCompile "org.hamcrest:hamcrest-core:$hamcrestVersion"
    testCompile "org.hamcrest:hamcrest-library:$hamcrestVersion"
    testCompile "org.mockito:mockito-core:$mockitoVersion"
    testCompile "org.springframework:spring-test:$springVersion"
    testCompile("javax.annotation:jsr250-api:1.0")
    testCompile "org.springframework.integration:spring-integration-stream:$springIntVersion"
    testCompile "org.springframework.integration:spring-integration-file:$springIntVersion"

    testRuntime "org.springframework.integration:spring-integration-event:$springIntVersion"
    
    testRuntime "org.codehaus.groovy:groovy:$groovyVersion"
    testRuntime "org.jruby:jruby:$jrubyVersion"
    testRuntime "org.python:jython-standalone:$jythonVersion"
    
    testCompile "cascading:cascading-local:$cascadingVersion"

    // Conflict resolution for dependency incompatibilities between Pig and Hive
    // Comment out the following two lines or specify a version of antlr that works 
    // with both the Hive and Pig versions in use. 
    // (for Hive 0.10.0 antlr 3.0.1 seems to work).
    testRuntime "org.antlr:antlr:$antlrVersion"
    testRuntime "org.antlr:antlr-runtime:$antlrVersion"
    
}

// exclude poms from the classpath (pulled in by Cloudera)
eclipse.classpath.file {
    whenMerged { classpath ->
        classpath.entries.removeAll { entry -> entry.toString().contains(".pom") }
    }
}

sourceCompatibility = 1.6
targetCompatibility = 1.6 

ext.skipPig = true
ext.skipHive = true
ext.skipHBase = true
ext.skipWebHdfs = true
ext.skipCascading = true

task enablePigTests {
    description = "Enable Pig tests"
    group = "Verification"
    
    doLast() {
        project.ext.skipPig = false
   }
}

task enableHiveTests {
    description = "Enable Hive tests"
    group = "Verification"
    doLast() {
        project.ext.skipHive = false
   }
}

task enableHBaseTests {
    description = "Enable HBase tests"
    group = "Verification"
    doLast() {
        project.ext.skipHBase = false
    }
}

task enableWebHdfsTests {
    description = "Enable WebHdfs tests"
    group = "Verification"
    doLast() {
        project.ext.skipWebHdfs = false
    }
}

task enableCascadingTests {
    description = "Enable Cascading tests"
    group = "Verification"
    doLast() {
        project.ext.skipCascading = false
    }
}

task enableAllTests() {
    description = "Enable all (incl. Pig, Hive, HBase, WebHdfs, Cascading) tests"
    group = "Verification"
    doFirst() {
      println "Enable all tests"
      project.ext.skipPig = false
      project.ext.skipHBase = false
      project.ext.skipHive = false
      project.ext.skipWebHdfs = false
      project.ext.skipCascading = false
    }
}

test {
    if (project.hasProperty('test.forkEvery')) {
        forkEvery = project.getProperty('test.forkEvery').toInteger()
    }
    systemProperties['input.path'] = 'build/classes/test/input'
    systemProperties['output.path'] = 'build/classes/test/output'
    includes = ["**/*.class"]

    testLogging {
        events "started"
        minGranularity 2
        maxGranularity 2
    }

    doFirst() {
        ext.msg = " "
        
        if (skipPig) {
            ext.msg += "Pig "
            excludes.add("**/pig/**")
        }
        if (skipHBase) {
            ext.msg += "HBase "
            excludes.add("**/hbase/**")
        }
        
        if (skipHive) {
            ext.msg += "Hive "
            excludes.add("**/hive/**")
        }

        if (skipWebHdfs) {
            ext.msg += "WebHdfs "
            excludes.add("**/WebHdfs*")
        }

        if (skipCascading) {
            ext.msg += "Cascading "
            excludes.add("**/cascading/**")
        }
        
        if (!msg.isEmpty())
            println "Skipping [$msg] Tests";
        
        // check prefix for hd.fs
        // first copy the properties since we can't change them
        ext.projProps = project.properties
        
        if (projProps.containsKey("hd.fs")) {
            String hdfs = projProps["hd.fs"].toString()
            if (!hdfs.contains("://")) {
                projProps.put("hd.fs", "hdfs://" + hdfs)
            }
        }
            
        // due to GRADLE-2475, set the system properties manually
        projProps.each { k,v ->
             if (k.toString().startsWith("hd.")) {
                systemProperties[k] = projProps[k]
             }
        }
    }
}

task downloadGutenbergBooks {
    ant.get(src: 'http://mirrors.xmission.com/gutenberg/1/0/100/100.txt',
            dest: 'src/test/resources/input/gutenberg',skipexisting:true)
    ant.get(src: 'http://mirrors.xmission.com/gutenberg/1/3/135/135.txt',
            dest: 'src/test/resources/input/gutenberg',skipexisting:true)
    ant.get(src: 'http://mirrors.xmission.com/gutenberg/1/3/9/1399/1399.txt',
            dest: 'src/test/resources/input/gutenberg',skipexisting:true)
    ant.get(src: 'http://mirrors.xmission.com/gutenberg/2/6/0/2600/2600.txt',
            dest: 'src/test/resources/input/gutenberg',skipexisting:true)
}

javadoc {
  ext.srcDir = file("${projectDir}/docs/src/api")
  
  configure(options) {
      stylesheetFile = file("${srcDir}/spring-javadoc.css")
      overview = "${srcDir}/overview.html"
      docFilesSubDirs = true
      outputLevel = org.gradle.external.javadoc.JavadocOutputLevel.QUIET
      breakIterator = true
      author = true
      showFromProtected()
      
     links = [
        "http://static.springframework.org/spring/docs/3.0.x/javadoc-api",
        "http://docs.oracle.com/javase/6/docs/api",
        "http://commons.apache.org/proper/commons-logging/apidocs/",
        "http://logging.apache.org/log4j/1.2/apidocs/",
        "http://hadoop.apache.org/common/docs/current/api/",
        "http://hbase.apache.org/apidocs/",
        "http://pig.apache.org/docs/r0.10.0/api/",
        "http://hive.apache.org/javadocs/r0.10.0/api/",
        "http://docs.spring.io/spring-batch/apidocs/",
        "http://docs.spring.io/spring-integration/api/",
        "https://builds.apache.org/job/Thrift/javadoc/",
        "http://docs.cascading.org/cascading/2.1/javadoc/"
     ]
     
     exclude "org/springframework/data/hadoop/config/**"
  }
    
  title = "${rootProject.description} ${version} API"
}

jar {
    manifest.attributes["Created-By"] = "${System.getProperty("java.version")} (${System.getProperty("java.specification.vendor")})"
    manifest.attributes['Implementation-Title'] = 'spring-data-hadoop'
    manifest.attributes['Implementation-Version'] = project.version
    manifest.attributes['Implementation-URL'] = "http://projects.spring.io/spring-hadoop/"
    manifest.attributes['Implementation-Vendor'] = "Spring by Pivotal"
    manifest.attributes['Implementation-Vendor-Id'] = "org.springframework"
    
    def build = System.env['SHDP.BUILD']
    if (build != null)
        manifest.attributes['Build'] = build
    
    String rev = "unknown"
    
    // parse the git files to find out the revision
    File gitHead = file('.git/HEAD')
    if (gitHead.exists()) {
        gitHead = file('.git/' + gitHead.text.trim().replace('ref: ',''))
        if (gitHead.exists()) { rev = gitHead.text }
    }

    from("$rootDir/docs/src/info") {
        include "license.txt"
        include "notice.txt"
        into "META-INF"
        expand(copyright: new Date().format('yyyy'), version: project.version)
    }

    manifest.attributes['Repository-Revision'] = rev
}

task sourcesJar(type: Jar, dependsOn:classes) {
    classifier = 'sources'
    from sourceSets.main.allJava
}

task javadocJar(type: Jar) {
    classifier = 'javadoc'
    from javadoc
}

reference {
    sourceDir = file('docs/src/reference/docbook')
    //pdfFileName = 'spring-data-hadoop-reference.pdf'
}


task docsZip(type: Zip) {
    group = 'Distribution'
    classifier = 'docs'
    description = "Builds -${classifier} archive containing api and reference for deployment"

    from('docs/src/info') {
        include 'changelog.txt'
    }

    from (javadoc) {
        into 'api'
    }

    from (reference) {
        into 'reference'
    }
}

task schemaZip(type: Zip) {
    group = 'Distribution'
    classifier = 'schema'
    description = "Builds -${classifier} archive containing all XSDs for deployment"

    def Properties schemas = new Properties();
    
    sourceSets.main.resources.find {
        it.path.endsWith('META-INF' + File.separator + 'spring.schemas')
    }?.withInputStream { schemas.load(it) }

    for (def key : schemas.keySet()) {
        def shortName = key.replaceAll(/http.*schema.(.*).spring-.*/, '$1')
        def alias = key.replaceAll(/http.*schema.(.*).(spring-.*)/, '$2')
        assert shortName != key
        File xsdFile = sourceSets.main.resources.find {
            it.path.replace('\\', '/').endsWith(schemas.get(key))
        }
        assert xsdFile != null
        
        into (shortName) {
           from xsdFile.path
           rename { String fileName -> alias }
        }
    }    
}

task distZip(type: Zip, dependsOn: [jar, docsZip, schemaZip, sourcesJar, javadocJar]) {
    group = 'Distribution'
    classifier = 'dist'
    description = "Builds -${classifier} archive, containing all jars and docs, " +
                  "suitable for community download page."

    ext.zipRootDir = "${project.name}-${project.version}"

    into (zipRootDir) {
        from('docs/src/info') {
            include 'readme.txt'
            include 'license.txt'
            include 'notice.txt'
            expand(copyright: new Date().format('yyyy'), version: project.version)
        }

        from('samples/') {
                into 'samples'
                exclude '**/build/**'
                exclude '**/bin/**'
                exclude '**/.settings/**'
                exclude '**/.gradle/**'
                exclude '**/.*'
        }
        
        from(zipTree(docsZip.archivePath)) {
            into "docs"
        }

        from(zipTree(schemaZip.archivePath)) {
            into "schema"
        }
        into ("dist") {
            from rootProject.collect { project -> project.libsDir }
        }
    }
}

artifacts {
    archives sourcesJar
    archives javadocJar

    archives docsZip
    archives schemaZip
    archives distZip
}

task wrapper(type: Wrapper) {
    description = 'Generates gradlew[.bat] scripts'
    gradleVersion = '1.9'
}

assemble.dependsOn = ['jar', 'sourcesJar']
defaultTasks 'build'
